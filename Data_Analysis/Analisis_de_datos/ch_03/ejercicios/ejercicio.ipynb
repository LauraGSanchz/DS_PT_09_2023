{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "\n",
    "Completa los siguientes ejercicios utilizando lo que hemos aprendido hasta y los datos del directorio ejercicios/:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Queremos ver los datos de las acciones de Facebook, Apple, Amazon, Netflix y Google (FAANG), pero nos dieron cada uno como un archivo CSV separado. Combínelos en un único archivo y guarde el dataframe de los datos FAANG como faang para el resto de los ejercicios:\n",
    "- a) Lee los archivos aapl.csv, amzn.csv, fb.csv, goog.csv y nflx.csv.\n",
    "- b) Añade una columna a cada dataframe, llamalo ticker, indicando el símbolo del ticker al que corresponde (el de Apple es AAPL, por ejemplo); así es como se busca una acción. En este caso, los nombres de los archivos son los símbolos de los teletipos.\n",
    "- c) Agréguelos en un dataframe.\n",
    "- d) Guarda el resultado en un archivo CSV llamado faang.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(color_codes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl = pd.read_csv('aapl.csv')\n",
    "fb = pd.read_csv('fb.csv')\n",
    "amzn = pd.read_csv('amzn.csv')\n",
    "ntflx = pd.read_csv('nflx.csv')\n",
    "ggl = pd.read_csv('goog.csv')\n",
    "\n",
    "lista_df = [appl, fb,amzn, ntflx, ggl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿cómo hacerlo con un bucle for?\n",
    "\n",
    "appl['ticker'] = 'AAPL'\n",
    "amzn['ticker'] = 'AMZN'\n",
    "fb['ticker'] = 'FB'\n",
    "ggl['ticker'] = 'GOOG'\n",
    "ntflx['ticker'] = 'NFLX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "faang = pd.merge(appl,amzn , how = 'outer')\n",
    "faang = pd.merge(faang, fb, how='outer')\n",
    "faang = pd.merge(faang, ggl, how='outer')\n",
    "faang = pd.merge(faang, ntflx, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>43.075001</td>\n",
       "      <td>42.314999</td>\n",
       "      <td>42.540001</td>\n",
       "      <td>43.064999</td>\n",
       "      <td>102223600.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>43.637501</td>\n",
       "      <td>42.990002</td>\n",
       "      <td>43.132500</td>\n",
       "      <td>43.057499</td>\n",
       "      <td>118071600.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>43.367500</td>\n",
       "      <td>43.020000</td>\n",
       "      <td>43.134998</td>\n",
       "      <td>43.257500</td>\n",
       "      <td>89738400.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>43.842499</td>\n",
       "      <td>43.262501</td>\n",
       "      <td>43.360001</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>94640000.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>43.902500</td>\n",
       "      <td>43.482498</td>\n",
       "      <td>43.587502</td>\n",
       "      <td>43.587502</td>\n",
       "      <td>82271200.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>250.649994</td>\n",
       "      <td>233.679993</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>233.880005</td>\n",
       "      <td>9547600.0</td>\n",
       "      <td>NFLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>254.500000</td>\n",
       "      <td>231.229996</td>\n",
       "      <td>233.919998</td>\n",
       "      <td>253.669998</td>\n",
       "      <td>14402700.0</td>\n",
       "      <td>NFLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>255.589996</td>\n",
       "      <td>240.100006</td>\n",
       "      <td>250.110001</td>\n",
       "      <td>255.570007</td>\n",
       "      <td>12235200.0</td>\n",
       "      <td>NFLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>261.910004</td>\n",
       "      <td>249.800003</td>\n",
       "      <td>257.940002</td>\n",
       "      <td>256.079987</td>\n",
       "      <td>10992800.0</td>\n",
       "      <td>NFLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>270.100006</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>260.160004</td>\n",
       "      <td>267.660004</td>\n",
       "      <td>13508900.0</td>\n",
       "      <td>NFLX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1255 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date        high         low        open       close       volume  \\\n",
       "0     2018-01-02   43.075001   42.314999   42.540001   43.064999  102223600.0   \n",
       "1     2018-01-03   43.637501   42.990002   43.132500   43.057499  118071600.0   \n",
       "2     2018-01-04   43.367500   43.020000   43.134998   43.257500   89738400.0   \n",
       "3     2018-01-05   43.842499   43.262501   43.360001   43.750000   94640000.0   \n",
       "4     2018-01-08   43.902500   43.482498   43.587502   43.587502   82271200.0   \n",
       "...          ...         ...         ...         ...         ...          ...   \n",
       "1250  2018-12-24  250.649994  233.679993  242.000000  233.880005    9547600.0   \n",
       "1251  2018-12-26  254.500000  231.229996  233.919998  253.669998   14402700.0   \n",
       "1252  2018-12-27  255.589996  240.100006  250.110001  255.570007   12235200.0   \n",
       "1253  2018-12-28  261.910004  249.800003  257.940002  256.079987   10992800.0   \n",
       "1254  2018-12-31  270.100006  260.000000  260.160004  267.660004   13508900.0   \n",
       "\n",
       "     ticker  \n",
       "0      AAPL  \n",
       "1      AAPL  \n",
       "2      AAPL  \n",
       "3      AAPL  \n",
       "4      AAPL  \n",
       "...     ...  \n",
       "1250   NFLX  \n",
       "1251   NFLX  \n",
       "1252   NFLX  \n",
       "1253   NFLX  \n",
       "1254   NFLX  \n",
       "\n",
       "[1255 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1255, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faang.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Con faang, utilice la conversión de tipos para convertir los valores de la columna de fecha en datetime y la columna de volumen en números enteros. A continuación, ordena por fecha y ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       object\n",
       "high      float64\n",
       "low       float64\n",
       "open      float64\n",
       "close     float64\n",
       "volume      int32\n",
       "ticker     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faang.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faang['volume'] = faang['volume'].astype('int')\n",
    "faang['volume'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Encuentre las siete filas en faang con el valor más bajo para el volumen.\n",
    "4. En este momento, los datos están entre formato largo y ancho. Utilice melt() para que el formato sea completamente largo. Sugerencia: fecha y ticker son nuestras variables ID\n",
    "(identifican de forma única cada fila). Necesitamos fundir(melt) el resto para no tener columnas separadas para apertura, máximo, mínimo, cierre y volumen.\n",
    "5. Supongamos que descubrimos que el 26 de julio de 2018 hubo un fallo en cómo se registraron los datos. ¿Cómo debemos manejar esto? Tenga en cuenta que no se requiere codificación para este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. El Centro Europeo para la Prevención y el Control de las Enfermedades (ECDC) proporciona un conjunto de datos abierto sobre casos de COVID-19 denominado número diario de nuevos casos notificados de COVID-19 por país en todo el mundo (https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographicdistribution-covid-19-cases-worldwide). Este conjunto de datos se actualiza diariamente, pero utilizaremos una instantánea que contiene datos desde el 1 de enero de 2020 hasta el 18 de septiembre de 2020. Limpia y pivota los datos para que estén en formato ancho:\n",
    "- a) Lea el archivo covid19_casos.csv.\n",
    "- b) Cree una columna de fecha utilizando los datos de la columna dateRep y la función pd.to_datetime().\n",
    "- c) Establece la columna de fecha como índice y ordena el índice.\n",
    "- d) Sustituye todas las apariciones de United_States_of_America y United_Kingdom por USA y UK, respectivamente. Sugerencia: el método replace() puede ejecutarse en todo el dataframe.\n",
    "- e) Utilizando la columna countriesAndTerritories, filtre los datos de casos COVID-19 depurados hasta Argentina, Brasil, China, Colombia, India, Italia, México, Perú, Rusia, España, Turquía, Reino Unido y Estados Unidos.\n",
    "- f) Pivota los datos de modo que el índice contenga las fechas, las columnas contengan los nombres de los países y los valores sean los recuentos de casos (la columna de casos). Asegúrate de rellenar los valores NaN con 0.\n",
    "7. Para determinar los totales de casos por país de forma eficiente, necesitamos las habilidades de agregación que aprenderemos en el Capítulo 4, Agregación de Pandas DataFrames, así que los datos ECDC en el fichero covid19_casos.csv han sido agregados para nosotros y guardados en el fichero covid19_total_casos.csv. Contiene el número total de casos por país. Utilice estos datos para encontrar los 20 países con los mayores totales de casos de COVID-19. Sugerencias: al leer el archivo CSV, introduzca index_col='cases' y tenga en cuenta que será útil transponer los datos antes de aislar los países."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
